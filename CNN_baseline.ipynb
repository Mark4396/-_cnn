{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastNLP\n",
    "import xml.dom.minidom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理XML文件到csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of train xml solve\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def TrainXML():\n",
    "    dom = xml.dom.minidom.parse('SMP2019_ECISA_Train.xml')\n",
    "    root = dom.documentElement\n",
    "    f = open('train.csv','w',encoding='utf-8')\n",
    "    csv_writer = csv.writer(f)\n",
    "    sentences = root.getElementsByTagName('Sentence')\n",
    "    for sentence in sentences:\n",
    "        label = sentence.getAttribute('label')\n",
    "        if label == '0':\n",
    "            csv_writer.writerow([\"0\",sentence.firstChild.data])\n",
    "            continue\n",
    "        elif label == '1':\n",
    "            csv_writer.writerow([\"1\",sentence.firstChild.data])\n",
    "            continue\n",
    "        elif label == '2':\n",
    "            csv_writer.writerow([\"2\",sentence.firstChild.data])\n",
    "            continue\n",
    "        else:  \n",
    "            # 没有label属性，无情感标注为0\n",
    "            #csv_writer.writerow([\"0\",sentence.firstChild.data])\n",
    "            continue\n",
    "TrainXML()\n",
    "print(\"part of train xml solve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of devxml solve\n"
     ]
    }
   ],
   "source": [
    "def DevXML():\n",
    "    dom = xml.dom.minidom.parse('SMP2019_ECISA_Dev.xml')\n",
    "    root = dom.documentElement\n",
    "    f = open('dev.csv','w',encoding='utf-8')\n",
    "    csv_writer = csv.writer(f)\n",
    "    sentences = root.getElementsByTagName('Sentence')\n",
    "    for sentence in sentences:\n",
    "        label = sentence.getAttribute('label')\n",
    "        if label == '0':\n",
    "            csv_writer.writerow([\"0\",sentence.firstChild.data])\n",
    "            continue\n",
    "        elif label == '1':\n",
    "            csv_writer.writerow([\"1\",sentence.firstChild.data])\n",
    "            continue\n",
    "        elif label == '2':\n",
    "            csv_writer.writerow([\"2\",sentence.firstChild.data])\n",
    "            continue\n",
    "        else:  \n",
    "            # 没有label属性，无情感标注为0\n",
    "            #csv_writer.writerow([\"0\",sentence.firstChild.data])\n",
    "            continue\n",
    "DevXML()\n",
    "print(\"part of devxml solve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of testxml solve\n"
     ]
    }
   ],
   "source": [
    "def TestXML():\n",
    "    dom = xml.dom.minidom.parse('SMP2019_ECISA_Test.xml')\n",
    "    root = dom.documentElement\n",
    "    f = open('test.csv','w',encoding='utf-8')\n",
    "    csv_writer = csv.writer(f)\n",
    "    sentences = root.getElementsByTagName('Sentence')\n",
    "    for sentence in sentences:\n",
    "        label = sentence.getAttribute('label')\n",
    "        if label == '0':\n",
    "            csv_writer.writerow([\"0\",sentence.firstChild.data])\n",
    "            continue\n",
    "        elif label == '1':\n",
    "            csv_writer.writerow([\"1\",sentence.firstChild.data])\n",
    "            continue\n",
    "        elif label == '2':\n",
    "            csv_writer.writerow([\"2\",sentence.firstChild.data])\n",
    "            continue\n",
    "        else:  \n",
    "            # 没有label属性，无情感标注为0\n",
    "            #csv_writer.writerow([\"0\",sentence.firstChild.data])\n",
    "            continue\n",
    "TestXML()\n",
    "print(\"part of testxml solve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load数据为databundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_pd = pd.read_csv(r'C:\\Users\\Rye\\Desktop\\cnn\\train.csv', sep = ',')\n",
    "dev_data_pd = pd.read_csv(r'C:\\Users\\Rye\\Desktop\\cnn\\dev.csv', sep = ',')\n",
    "test_data_pd = pd.read_csv(r'C:\\Users\\Rye\\Desktop\\cnn\\test.csv', sep = ',')\n",
    "\n",
    "train_data_pd.to_csv('train.txt',sep=',', index=False,header=None,encoding='utf-8')\n",
    "dev_data_pd.to_csv('dev.txt',sep=',', index=False,header=None,encoding='utf-8')\n",
    "test_data_pd.to_csv('test.txt',sep=',', index=False,header=None,encoding='utf-8')\n",
    "\n",
    "#windows下必须转化成txt才能读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------------------+\n",
      "| target | raw_words                           |\n",
      "+--------+-------------------------------------+\n",
      "| 2      | 王健林把万达广场也卖了，万达都快... |\n",
      "| 2      | 【易到司机提现延期背后:或因乐视...  |\n",
      "| 1      | 建行调乐视员工信用卡额度至1元排...  |\n",
      "| 1      | 这是一个蛮有意思的小岛，距离香港... |\n",
      "| 2      | 夜半撞车我的评分：符合自己风格的... |\n",
      "| 2      | 看到北京工作的同学这时发的雾霾图... |\n",
      "| 2      | #我为都教授看春晚#我宁愿为赵本山... |\n",
      "| 2      | 阿姨被淘汰我没话说，但是里斯蛋泥... |\n",
      "| 1      | 如果我没吃完的话……哈哈哈…吃完...    |\n",
      "| 0      | 馆内餐饮：馆内有咖啡厅、面包店等... |\n",
      "+--------+-------------------------------------+\n",
      "14773 5142 3808\n",
      "In total 3 datasets:\n",
      "\tdev has 5142 instances.\n",
      "\ttest has 3808 instances.\n",
      "\ttrain has 14773 instances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.io.loader import CSVLoader\n",
    "data_set_loader = CSVLoader(\n",
    "    headers=('target','raw_words'), sep=','\n",
    ")\n",
    "import os\n",
    "os.remove('C:/Users/Rye/Desktop/cnn/train.csv') \n",
    "os.remove('C:/Users/Rye/Desktop/cnn/test.csv') \n",
    "os.remove('C:/Users/Rye/Desktop/cnn/dev.csv') \n",
    "data_bundle = data_set_loader.load('C:/Users/Rye/Desktop/cnn')\n",
    "\n",
    "train_data = data_bundle.get_dataset('train')\n",
    "dev_data = data_bundle.get_dataset('dev')\n",
    "test_data = data_bundle.get_dataset('test')\n",
    "\n",
    "data_bundle.set_dataset(test_data, 'test')\n",
    "data_bundle.set_dataset(train_data, 'train')\n",
    "print(data_bundle.get_dataset('test')[:10])\n",
    "print(len(train_data),len(dev_data),len(test_data))\n",
    "print(data_bundle)\n",
    "#懒得加代码了，必须手动删除csv文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用DataSet预处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------+---------------------+---------+\n",
      "| target | raw_words            | words               | seq_len |\n",
      "+--------+----------------------+---------------------+---------+\n",
      "| 2      | 王健林把万达广场...  | ['王', '健', '林... | 20      |\n",
      "| 2      | 【易到司机提现延...  | ['【', '易', '到... | 68      |\n",
      "| 1      | 建行调乐视员工信...  | ['建', '行', '调... | 25      |\n",
      "| 1      | 这是一个蛮有意思...  | ['这', '是', '一... | 29      |\n",
      "| 2      | 夜半撞车我的评分...  | ['夜', '半', '撞... | 42      |\n",
      "| 2      | 看到北京工作的同...  | ['看', '到', '北... | 130     |\n",
      "| 2      | #我为都教授看春晚... | ['#', '我', '为'... | 83      |\n",
      "| 2      | 阿姨被淘汰我没话...  | ['阿', '姨', '被... | 25      |\n",
      "| 1      | 如果我没吃完的话...  | ['如', '果', '我... | 24      |\n",
      "| 0      | 馆内餐饮：馆内有...  | ['馆', '内', '餐... | 44      |\n",
      "+--------+----------------------+---------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "from fastNLP import DataSet\n",
    "import re\n",
    "def get_words(instance):\n",
    "    sentence = instance['raw_words']\n",
    "    # 首先分割 英文 以及英文和标点\n",
    "    pattern_char_1 = re.compile(r'([\\W])')\n",
    "    parts = pattern_char_1.split(sentence)\n",
    "    parts = [p for p in parts if len(p.strip())>0]\n",
    "    # 分割中文\n",
    "    pattern = re.compile(r'([\\u4e00-\\u9fa5])')\n",
    "    words = pattern.split(sentence)\n",
    "    words = [w for w in words if len(w.strip())>0]\n",
    "    return words\n",
    "train_data.apply(get_words, new_field_name='words')\n",
    "dev_data.apply(get_words, new_field_name='words')\n",
    "test_data.apply(get_words, new_field_name='words')\n",
    "\n",
    "def get_len(instance):\n",
    "    sentence = instance['words']\n",
    "    return len(sentence)\n",
    "train_data.apply(get_len, new_field_name='seq_len')\n",
    "dev_data.apply(get_len, new_field_name='seq_len')\n",
    "test_data.apply(get_len, new_field_name='seq_len')\n",
    "\n",
    "\n",
    "print(data_bundle.get_dataset('test')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Vocabulary转换文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary(['//@nokki', '卡', '布', '诺', '琪']...)\n",
      "In total 3 datasets:\n",
      "\tdev has 5142 instances.\n",
      "\ttest has 3808 instances.\n",
      "\ttrain has 14773 instances.\n",
      "In total 2 vocabs:\n",
      "\twords has 12633 entries.\n",
      "\ttarget has 3 entries.\n",
      "\n",
      "+--------+----------------------+---------------------+---------+\n",
      "| target | raw_words            | words               | seq_len |\n",
      "+--------+----------------------+---------------------+---------+\n",
      "| 1      | 王健林把万达广场...  | [150, 992, 290, ... | 20      |\n",
      "| 1      | 【易到司机提现延...  | [201, 718, 19, 1... | 68      |\n",
      "| 2      | 建行调乐视员工信...  | [129, 68, 595, 4... | 25      |\n",
      "| 2      | 这是一个蛮有意思...  | [15, 5, 8, 13, 1... | 29      |\n",
      "| 1      | 夜半撞车我的评分...  | [420, 335, 1851,... | 42      |\n",
      "| 1      | 看到北京工作的同...  | [21, 19, 73, 90,... | 130     |\n",
      "| 1      | #我为都教授看春晚... | [462, 7, 42, 325... | 83      |\n",
      "| 1      | 阿姨被淘汰我没话...  | [306, 1714, 107,... | 25      |\n",
      "| 2      | 如果我没吃完的话...  | [185, 208, 7, 37... | 24      |\n",
      "| 0      | 馆内餐饮：馆内有...  | [295, 267, 735, ... | 44      |\n",
      "+--------+----------------------+---------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "from fastNLP import Vocabulary\n",
    "vocab = Vocabulary()\n",
    "vocab.from_dataset(train_data, dev_data, test_data, field_name='words')\n",
    "vocab.index_dataset(train_data, dev_data, test_data, field_name='words')\n",
    "data_bundle.set_vocab(vocab, 'words')\n",
    "char_vocab = data_bundle.get_vocab('words')\n",
    "#char_vocab.to_index('讨')\n",
    "print(char_vocab)\n",
    "#print(data_bundle)\n",
    "\n",
    "target_vocab = Vocabulary(padding=None, unknown=None)\n",
    "target_vocab.from_dataset(train_data, dev_data, test_data, field_name='target')\n",
    "target_vocab.index_dataset(train_data, dev_data, test_data, field_name='target')\n",
    "data_bundle.set_vocab(target_vocab, 'target')\n",
    "print(data_bundle)\n",
    "print(data_bundle.get_dataset('test')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Embedding模块将文本转成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4820 out of 12633 words in the pre-training embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fastNLP.embeddings import StaticEmbedding\n",
    "\n",
    "word2vec_embed = StaticEmbedding(char_vocab, model_dir_or_name='cn-char-fastnlp-100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-----------+-------+---------+\n",
      "| field_names | target | raw_words | words | seq_len |\n",
      "+-------------+--------+-----------+-------+---------+\n",
      "|   is_input  | False  |   False   |  True |   True  |\n",
      "|  is_target  |  True  |   False   | False |  False  |\n",
      "| ignore_type | False  |           | False |  False  |\n",
      "|  pad_value  |   0    |           |   0   |    0    |\n",
      "+-------------+--------+-----------+-------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<prettytable.PrettyTable at 0x2035609eec8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastNLP.io import CSVLoader\n",
    "from fastNLP import Vocabulary, CrossEntropyLoss, AccuracyMetric, ClassifyFPreRecMetric\n",
    "from fastNLP.models import CNNText\n",
    "from fastNLP import Trainer\n",
    "from fastNLP import Tester\n",
    "\n",
    "train_data.set_input('words','seq_len')\n",
    "train_data.set_target('target')\n",
    "dev_data.set_input('words','seq_len')\n",
    "dev_data.set_target('target')\n",
    "test_data.set_input('words','seq_len')\n",
    "test_data.set_target('target')\n",
    "test_data.print_field_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNText(\n",
      "  (embed): Embedding(\n",
      "    (embed): StaticEmbedding(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (embedding): Embedding(12633, 100, padding_idx=0)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (conv_pool): ConvMaxpool(\n",
      "    (convs): ModuleList(\n",
      "      (0): Conv1d(100, 30, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): Conv1d(100, 40, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      (2): Conv1d(100, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (fc): Linear(in_features=120, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_cnn = CNNText(word2vec_embed, num_classes=3, dropout=0)\n",
    "print(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from fastNLP import MetricBase\n",
    "import numpy as np\n",
    "\n",
    "class FMetric(MetricBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 根据你的情况自定义指标\n",
    "        self.ture = []\n",
    "        self.pre = []\n",
    "\n",
    "    def evaluate(self, target, pred): # 这里的名称需要和dataset中target field与model返回的key是一样的，不然找不到对应的value\n",
    "        # dev或test时，每个batch结束会调用一次该方法，需要实现如何根据每个batch累加metric\n",
    "        #tp = torch.max(pred,1)[1].view(target.size()).data.cpu().numpy() #[1,3]\n",
    "        #tt = target.data.cpu().numpy()\n",
    "        fp = pred.data.cpu().numpy()\n",
    "     #  print(pred)\n",
    "     #   print(target.size())\n",
    "        ft = target.data.cpu().numpy()\n",
    "        tp = np.ndarray.tolist(fp)\n",
    "        tt = np.ndarray.tolist(ft)\n",
    "        \n",
    "        self.ture.extend(tt)\n",
    "        self.pre.extend(tp)\n",
    "        #print(type(tp))\n",
    "        #print(tp)\n",
    "        #f1s = f1_score(tt, tp, average='macro' ) #计算每个batch的f1-score\n",
    "        # report = classification_report(tt, tp, labels=[0,1,2])\n",
    "        \n",
    "        #self.total += target.size(0)\n",
    "        #self.f1_count += f1s * target.size(0)\n",
    "\n",
    "\n",
    "    def get_metric(self, reset=True): # 在这里定义如何计算metric\n",
    "        # f1 = self.f1_count / self.total\n",
    "        report = classification_report(self.ture, self.pre, labels=[0,1,2])\n",
    "        if reset: # 是否清零以便重新计算\n",
    "            #self.f1_count = 0\n",
    "            #self.total = 0\n",
    "            self.ture.clear()\n",
    "            self.pre.clear()\n",
    "        return {'Report': report}  # 需要返回一个dict，key为该metric的名称，该名称会显示到Trainer的progress bar中\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 1):\n",
      "\twords: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1, 15]) \n",
      "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1]) \n",
      "target fields after batch(if batch size is 1):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([1]) \n",
      "\n",
      "training epochs started 2020-07-10-15-44-17-055037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rye\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Rye\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=147730.0), HTML(value='')), layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 32.32 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 1/10. Step:14773/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      2553\n",
      "           1       0.78      0.66      0.72      1358\n",
      "           2       0.59      0.65      0.62      1231\n",
      "\n",
      "    accuracy                           0.76      5142\n",
      "   macro avg       0.73      0.72      0.73      5142\n",
      "weighted avg       0.76      0.76      0.76      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 32.84 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 2/10. Step:29546/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      2553\n",
      "           1       0.72      0.74      0.73      1358\n",
      "           2       0.63      0.60      0.61      1231\n",
      "\n",
      "    accuracy                           0.76      5142\n",
      "   macro avg       0.73      0.73      0.73      5142\n",
      "weighted avg       0.76      0.76      0.76      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 32.17 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 3/10. Step:44319/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      2553\n",
      "           1       0.65      0.77      0.71      1358\n",
      "           2       0.58      0.55      0.56      1231\n",
      "\n",
      "    accuracy                           0.72      5142\n",
      "   macro avg       0.69      0.70      0.69      5142\n",
      "weighted avg       0.73      0.72      0.73      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 32.39 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 4/10. Step:59092/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      2553\n",
      "           1       0.72      0.66      0.69      1358\n",
      "           2       0.56      0.62      0.59      1231\n",
      "\n",
      "    accuracy                           0.73      5142\n",
      "   macro avg       0.70      0.70      0.70      5142\n",
      "weighted avg       0.73      0.73      0.73      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 23.87 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 5/10. Step:73865/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81      2553\n",
      "           1       0.70      0.66      0.68      1358\n",
      "           2       0.53      0.62      0.57      1231\n",
      "\n",
      "    accuracy                           0.71      5142\n",
      "   macro avg       0.69      0.69      0.69      5142\n",
      "weighted avg       0.72      0.71      0.72      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 32.38 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 6/10. Step:88638/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      2553\n",
      "           1       0.67      0.74      0.70      1358\n",
      "           2       0.60      0.49      0.54      1231\n",
      "\n",
      "    accuracy                           0.72      5142\n",
      "   macro avg       0.69      0.69      0.69      5142\n",
      "weighted avg       0.72      0.72      0.72      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 32.62 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 7/10. Step:103411/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80      2553\n",
      "           1       0.72      0.61      0.66      1358\n",
      "           2       0.52      0.65      0.58      1231\n",
      "\n",
      "    accuracy                           0.71      5142\n",
      "   macro avg       0.68      0.68      0.68      5142\n",
      "weighted avg       0.72      0.71      0.71      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 30.82 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 8/10. Step:118184/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81      2553\n",
      "           1       0.70      0.66      0.68      1358\n",
      "           2       0.57      0.59      0.58      1231\n",
      "\n",
      "    accuracy                           0.72      5142\n",
      "   macro avg       0.69      0.69      0.69      5142\n",
      "weighted avg       0.72      0.72      0.72      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 30.67 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 9/10. Step:132957/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82      2553\n",
      "           1       0.68      0.72      0.70      1358\n",
      "           2       0.60      0.54      0.57      1231\n",
      "\n",
      "    accuracy                           0.73      5142\n",
      "   macro avg       0.70      0.69      0.69      5142\n",
      "weighted avg       0.73      0.73      0.73      5142\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5142.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 30.25 seconds!\n",
      "\r",
      "Evaluation on dev at Epoch 10/10. Step:147730/147730: \n",
      "\r",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      2553\n",
      "           1       0.69      0.69      0.69      1358\n",
      "           2       0.58      0.58      0.58      1231\n",
      "\n",
      "    accuracy                           0.72      5142\n",
      "   macro avg       0.69      0.69      0.69      5142\n",
      "weighted avg       0.72      0.72      0.72      5142\n",
      "\n",
      "\n",
      "\r",
      "Reloaded the best model.\n",
      "\n",
      "In Epoch:3/Step:44319, got best dev performance:\n",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      2553\n",
      "           1       0.65      0.77      0.71      1358\n",
      "           2       0.58      0.55      0.56      1231\n",
      "\n",
      "    accuracy                           0.72      5142\n",
      "   macro avg       0.69      0.70      0.69      5142\n",
      "weighted avg       0.73      0.72      0.73      5142\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_eval': {'FMetric': {'Report': '              precision    recall  f1-score   support\\n\\n           0       0.85      0.78      0.81      2553\\n           1       0.65      0.77      0.71      1358\\n           2       0.58      0.55      0.56      1231\\n\\n    accuracy                           0.72      5142\\n   macro avg       0.69      0.70      0.69      5142\\nweighted avg       0.73      0.72      0.73      5142\\n'}},\n",
       " 'best_epoch': 3,\n",
       " 'best_step': 44319,\n",
       " 'seconds': 2616.53}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.optim import Adam\n",
    "from fastNLP import CrossEntropyLoss\n",
    "loss = CrossEntropyLoss()\n",
    "optimizer = Adam(model_cnn.parameters(), lr=0.001)\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'  # 如果有gpu的话在gpu上运行，训练速度会更快\n",
    "metric = FMetric()\n",
    "trainer_cnn = Trainer(train_data=data_bundle.get_dataset('train'), model=model_cnn, loss=loss,\n",
    "                  optimizer=optimizer, batch_size=1, dev_data=data_bundle.get_dataset('dev'),\n",
    "                  metrics=metric,device=device)\n",
    "trainer_cnn.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=3808.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 24.0 seconds!\n",
      "[tester] \n",
      "FMetric: Report=              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77      1910\n",
      "           1       0.63      0.80      0.71       978\n",
      "           2       0.57      0.57      0.57       920\n",
      "\n",
      "    accuracy                           0.70      3808\n",
      "   macro avg       0.68      0.69      0.68      3808\n",
      "weighted avg       0.72      0.70      0.70      3808\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FMetric': {'Report': '              precision    recall  f1-score   support\\n\\n           0       0.83      0.71      0.77      1910\\n           1       0.63      0.80      0.71       978\\n           2       0.57      0.57      0.57       920\\n\\n    accuracy                           0.70      3808\\n   macro avg       0.68      0.69      0.68      3808\\nweighted avg       0.72      0.70      0.70      3808\\n'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cnn = Tester(data_bundle.get_dataset('test'), model=model_cnn,\n",
    "                   batch_size=1,\n",
    "                  metrics=metric)\n",
    "test_cnn.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#device = 0 if torch.cuda.is_available() else 'cpu'  # 如果有gpu的话在gpu上运行，训练速度会更快\\n#print(device)\\ntrainer_cnn = Trainer(model=model_cnn, train_data=train_data, dev_data=dev_data,loss=CrossEntropyLoss(), metrics=AccuracyMetric())\\n\\ntrainer_cnn.train()\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#device = 0 if torch.cuda.is_available() else 'cpu'  # 如果有gpu的话在gpu上运行，训练速度会更快\n",
    "#print(device)\n",
    "trainer_cnn = Trainer(model=model_cnn, train_data=train_data, dev_data=dev_data,loss=CrossEntropyLoss(), metrics=AccuracyMetric())\n",
    "\n",
    "trainer_cnn.train()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fastNLP.core.trainer.Trainer object at 0x000002035A1CCB08>\n"
     ]
    }
   ],
   "source": [
    "print(trainer_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
